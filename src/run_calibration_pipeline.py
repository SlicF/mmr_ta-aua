# -*- coding: utf-8 -*-
"""
Pipeline Completo de Calibra√ß√£o e Valida√ß√£o

EXECUTA:
========
1. Calibra√ß√£o: Aprende par√¢metros de dados hist√≥ricos
2. Previs√£o: Gera forecasts com par√¢metros fixos (baseline)
3. Valida√ß√£o: Compara fixo vs calibrado usando backtest
4. Relat√≥rio: Gera compara√ß√£o visual das melhorias

USO:
====
    python run_calibration_pipeline.py
    python run_calibration_pipeline.py --modalidade "FUTSAL MASCULINO"
    python run_calibration_pipeline.py --skip-calibration  # usar calibra√ß√£o existente
"""

import argparse
import subprocess
import sys
from pathlib import Path
import json
import time

REPO_ROOT = Path(__file__).resolve().parents[1]


def run_command(cmd: list, description: str, optional: bool = False) -> bool:
    """
    Executa comando e reporta status.

    Args:
        cmd: Lista com comando e argumentos
        description: Descri√ß√£o da opera√ß√£o
        optional: Se True, n√£o abortar em caso de erro

    Returns:
        True se sucesso, False se erro
    """
    print(f"\n{'='*70}")
    print(f"‚ñ∂  {description}")
    print(f"{'='*70}")
    print(f"   Comando: {' '.join(cmd)}")
    print()

    try:
        result = subprocess.run(
            cmd, cwd=REPO_ROOT / "src", check=True, capture_output=False, text=True
        )
        print(f"\n‚úÖ {description} - CONCLU√çDO")
        return True

    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå {description} - ERRO (exit code {e.returncode})")
        if not optional:
            print("\n‚ö†Ô∏è  Pipeline interrompido devido a erro cr√≠tico")
            sys.exit(1)
        return False

    except Exception as e:
        print(f"\n‚ùå {description} - EXCE√á√ÉO: {e}")
        if not optional:
            sys.exit(1)
        return False


def check_calibration_output() -> bool:
    """Verifica se ficheiros de calibra√ß√£o existem."""
    calibration_dir = REPO_ROOT / "docs" / "output" / "calibration"
    required_files = [
        calibration_dir / "calibrated_params_full.json",
        calibration_dir / "calibrated_simulator_config.json",
    ]

    all_exist = all(f.exists() for f in required_files)

    if all_exist:
        print("\n‚úì Ficheiros de calibra√ß√£o encontrados:")
        for f in required_files:
            print(f"   ‚Ä¢ {f.relative_to(REPO_ROOT)}")
    else:
        print("\n‚úó Ficheiros de calibra√ß√£o em falta:")
        for f in required_files:
            status = "‚úì" if f.exists() else "‚úó"
            print(f"   {status} {f.relative_to(REPO_ROOT)}")

    return all_exist


def generate_summary_report(modalidade: str = None):
    """Gera relat√≥rio resumo da pipeline."""
    print(f"\n{'='*70}")
    print("üìä RELAT√ìRIO FINAL DA PIPELINE")
    print(f"{'='*70}")

    calibration_dir = REPO_ROOT / "docs" / "output" / "calibration"

    # 1. Status calibra√ß√£o
    print("\n[1] STATUS DA CALIBRA√á√ÉO")
    print("‚îÄ" * 70)
    if check_calibration_output():
        config_file = calibration_dir / "calibrated_simulator_config.json"
        try:
            with open(config_file, "r", encoding="utf-8") as f:
                config = json.load(f)

            print(f"   Modalidades calibradas: {len(config)}")
            for mod in sorted(config.keys()):
                params = config[mod]
                print(f"   ‚Ä¢ {mod}:")
                print(
                    f"       base_goals={params.get('base_goals', 'N/A'):.2f}, "
                    f"dispersion_k={params.get('dispersion_k', 'N/A'):.2f}, "
                    f"draw_rate={params.get('base_draw_rate', 'N/A'):.1%}"
                )
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erro ao ler config: {e}")
    else:
        print("   ‚ùå Calibra√ß√£o n√£o conclu√≠da")

    # 2. Status backtest
    print("\n[2] STATUS DO BACKTEST")
    print("‚îÄ" * 70)
    backtest_dir = REPO_ROOT / "docs" / "output" / "elo_ratings"
    backtest_files = list(backtest_dir.glob("backtest_summary_*.json"))

    if backtest_files:
        print(f"   Backtests dispon√≠veis: {len(backtest_files)}")
        for bf in sorted(backtest_files):
            try:
                with open(bf, "r", encoding="utf-8") as f:
                    summary = json.load(f)
                mod = summary.get("modalidade", "?")
                brier = summary.get("avg_brier_score", 0)
                rmse = summary.get("avg_rmse_place", 0)
                print(f"   ‚Ä¢ {mod}: Brier={brier:.4f}, RMSE Place={rmse:.2f}")
            except Exception:
                continue
    else:
        print("   ‚ö†Ô∏è  Nenhum backtest executado ainda")

    # 3. Status compara√ß√£o
    print("\n[3] STATUS DA COMPARA√á√ÉO (FIXO vs CALIBRADO)")
    print("‚îÄ" * 70)
    comparison_files = list(calibration_dir.glob("comparison_*.json"))

    if comparison_files:
        print(f"   Compara√ß√µes dispon√≠veis: {len(comparison_files)}")
        for cf in sorted(comparison_files):
            print(f"   ‚Ä¢ {cf.name}")
            try:
                with open(cf, "r", encoding="utf-8") as f:
                    comp = json.load(f)
                status = comp.get("status", "unknown")
                print(f"       Status: {status}")
                if "fixed_model" in comp and "results" in comp["fixed_model"]:
                    res = comp["fixed_model"]["results"]
                    print(f"       Modelo Fixo: Brier={res.get('brier_score', 'N/A')}")
            except Exception as e:
                print(f"       ‚ö†Ô∏è  Erro ao ler: {e}")
    else:
        print("   ‚ö†Ô∏è  Nenhuma compara√ß√£o executada ainda")
        print("   üí° Para ativar compara√ß√£o completa:")
        print("      1. Integrar calibrated_params no preditor.py")
        print(
            '      2. Re-executar: python backtest_validation.py --compare-calibrated --modalidade "..."'
        )

    # 4. Pr√≥ximos passos
    print(f"\n{'='*70}")
    print("üéØ PR√ìXIMOS PASSOS")
    print(f"{'='*70}")
    print("\n1. INTEGRAR PAR√ÇMETROS CALIBRADOS NO PREDITOR:")
    print("   ‚Ä¢ Modificar SportScoreSimulator.__init__() para aceitar custom_params")
    print("   ‚Ä¢ Adicionar flag --use-calibrated no preditor.py")
    print("   ‚Ä¢ Carregar params do calibrated_simulator_config.json")

    print("\n2. RE-EXECUTAR COMPARA√á√ÉO:")
    print(
        '   python backtest_validation.py --compare-calibrated --modalidade "FUTSAL MASCULINO"'
    )

    print("\n3. ITERAR E MELHORAR:")
    print("   ‚Ä¢ Analisar m√©tricas de melhoria")
    print("   ‚Ä¢ Ajustar calibra√ß√£o se necess√°rio")
    print("   ‚Ä¢ Validar em produ√ß√£o")

    print(f"\n{'='*70}")


def main():
    parser = argparse.ArgumentParser(
        description="Pipeline completo de calibra√ß√£o e valida√ß√£o"
    )
    parser.add_argument(
        "--modalidade",
        type=str,
        help="Modalidade espec√≠fica (ex.: 'FUTSAL MASCULINO'). Se omitida, processa todas.",
    )
    parser.add_argument(
        "--skip-calibration",
        action="store_true",
        help="Pular etapa de calibra√ß√£o (usar calibra√ß√£o existente)",
    )
    parser.add_argument(
        "--skip-backtest",
        action="store_true",
        help="Pular backtest (apenas calibrar)",
    )
    parser.add_argument(
        "--season",
        type=str,
        help="√âpoca espec√≠fica para compara√ß√£o (ex.: '24_25')",
    )
    args = parser.parse_args()

    print("=" * 70)
    print("üöÄ PIPELINE DE CALIBRA√á√ÉO E VALIDA√á√ÉO")
    print("=" * 70)
    print(f"Modalidade: {args.modalidade or 'TODAS'}")
    print(f"√âpoca: {args.season or 'Mais recente'}")
    print("=" * 70)

    start_time = time.time()

    # STEP 1: CALIBRA√á√ÉO
    if not args.skip_calibration:
        run_command(
            ["python", "calibrator.py"],
            "STEP 1: Calibra√ß√£o de par√¢metros",
            optional=False,
        )
    else:
        print("\n‚è≠Ô∏è  PULANDO calibra√ß√£o (usando ficheiros existentes)")
        if not check_calibration_output():
            print("‚ùå Ficheiros de calibra√ß√£o n√£o encontrados!")
            print("   Remover --skip-calibration ou executar calibrator.py primeiro")
            sys.exit(1)

    # STEP 2: BACKTEST
    if not args.skip_backtest:
        backtest_cmd = ["python", "backtest_validation.py"]
        if args.modalidade:
            backtest_cmd.extend(["--modalidade", args.modalidade])

        run_command(
            backtest_cmd,
            "STEP 2: Backtest com modelo fixo (baseline)",
            optional=True,  # N√£o cr√≠tico se falhar
        )
    else:
        print("\n‚è≠Ô∏è  PULANDO backtest")

    # STEP 3: COMPARA√á√ÉO (se modalidade espec√≠fica)
    if args.modalidade and not args.skip_backtest:
        comparison_cmd = [
            "python",
            "backtest_validation.py",
            "--compare-calibrated",
            "--modalidade",
            args.modalidade,
        ]
        if args.season:
            comparison_cmd.extend(["--season", args.season])

        run_command(
            comparison_cmd,
            "STEP 3: Compara√ß√£o fixo vs calibrado",
            optional=True,  # Ainda n√£o totalmente implementado
        )

    # STEP 4: RELAT√ìRIO FINAL
    generate_summary_report(args.modalidade)

    elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è  Tempo total: {elapsed:.1f}s")
    print("=" * 70)


if __name__ == "__main__":
    main()
